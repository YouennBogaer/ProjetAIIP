{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acdb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import lvis\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e1043",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lvis import LVIS\n",
    "\n",
    "class FilteredLVIS(LVIS):\n",
    "    \"\"\"\n",
    "    Lighter version of LVIS which drops any unwanted data, filters over specific category ids,\n",
    "    keeping only annotations of these categories and images containing these annotations.\n",
    "    For RAM efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotation_path, cat_names):\n",
    "        super().__init__(annotation_path)\n",
    "        cat_ids = self._get_cat_ids(cat_names)\n",
    "        self.trim(cat_ids)\n",
    "\n",
    "    def trim(self, cat_ids):\n",
    "        cat_ids = set(cat_ids)\n",
    "        ann_ids = set(self.get_ann_ids(cat_ids=cat_ids))\n",
    "        img_ids = set(self._get_img_ids(cat_ids))\n",
    "        self.anns = {k: v for k, v in self.anns.items() if k in ann_ids}\n",
    "        self.cats = {k: v for k, v in self.cats.items() if k in cat_ids}\n",
    "        self.imgs = {k: v for k, v in self.imgs.items() if k in img_ids}\n",
    "        self.img_ann_map = {\n",
    "            iid: [ann for ann in anns if ann['id'] in ann_ids] \n",
    "            for iid, anns in self.img_ann_map.items() \n",
    "            if iid in img_ids\n",
    "        }\n",
    "        self.cat_img_map = {\n",
    "            cid: [iid for iid in imgs if iid in img_ids] \n",
    "            for cid, imgs in self.cat_img_map.items() \n",
    "            if cid in cat_ids\n",
    "        }\n",
    "        self.dataset = {}\n",
    "        self.dataset['annotations'] = [ann for ann in self.anns]\n",
    "        self.dataset['images'] = [img for img in self.imgs]\n",
    "        self.dataset['categories'] = [cat for cat in self.cats]\n",
    "\n",
    "    def _get_img_ids(self, cat_ids):\n",
    "        return list({\n",
    "            iid for cat_id in cat_ids \n",
    "            for iid in self.cat_img_map[cat_id]\n",
    "        })\n",
    "    \n",
    "    def _get_cat_ids(self, names):\n",
    "        cat_ids = []\n",
    "        cat_names = []\n",
    "        for id, cat in self.cats.items():\n",
    "            if cat['name'] in names:\n",
    "                cat_ids.append(id)\n",
    "                cat_names.append(cat['name'])\n",
    "        print(f'category found for {[name for name in names if name in cat_names]}')\n",
    "        print(f'category NOT found for {[name for name in names if name not in cat_names]}\\n')\n",
    "        return list(set(cat_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb267844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.ops import box_convert\n",
    "from pycocotools import mask as maskUtils\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "class LVISDataset(Dataset):\n",
    "    def __init__(self, coco2017_dirs: List[str], lvis_gt: lvis.LVIS|FilteredLVIS, transforms=None, cat_ids=None) -> None:\n",
    "        self.images_dirs = coco2017_dirs\n",
    "        self.lvis_gt = lvis_gt\n",
    "        self.transforms = transforms\n",
    "        self.img_ids = self.lvis_gt.get_img_ids() if cat_ids is None else self._get_img_ids(cat_ids)\n",
    "        self.cat_ids = self.lvis_gt.get_cat_ids() if cat_ids is None else cat_ids\n",
    "        self.cat_id_to_label = {cat_id: i+1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "        if  all(os.path.isdir(dir) for dir in self.images_dirs):\n",
    "            self._get_image = self._get_image_from_file\n",
    "            print(\"will load images from files\")\n",
    "        else:\n",
    "            self._get_image = self._get_image_from_url\n",
    "            print(\"will load images from urls\")\n",
    "\n",
    "\n",
    "    def _get_img_ids(self, cat_ids):\n",
    "        return list({\n",
    "            iid for cat_id in cat_ids \n",
    "            for iid in self.lvis_gt.cat_img_map[cat_id]\n",
    "        })\n",
    "    \n",
    "    def _get_image_from_file(self, id):\n",
    "        image_paths = [os.path.join(images_dir, f'{str(id).zfill(12)}.jpg') for images_dir in self.images_dirs]\n",
    "        for image_path in image_paths:\n",
    "            if os.path.isfile(image_path):\n",
    "                return decode_image(image_path)\n",
    "        print(f\"image not found: {image_paths}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    def _get_image_from_url(self, id):\n",
    "        url = self.lvis_gt.imgs[id]['coco_url']\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ### get image\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = self._get_image(img_id)\n",
    "        img = tv_tensors.Image(img)\n",
    "        _,h,w = img.shape\n",
    "\n",
    "        ### get annotationss\n",
    "        annot_ids = self.lvis_gt.get_ann_ids(img_ids=[img_id])\n",
    "        annots = self.lvis_gt.load_anns(annot_ids)\n",
    "        annots = [annot for annot in annots if annot['category_id'] in self.cat_ids]\n",
    "        # labels\n",
    "        labels = torch.tensor([self.cat_id_to_label[annot['category_id']] for annot in annots])\n",
    "        # area\n",
    "        areas = torch.tensor([annot['area'] for annot in annots])\n",
    "        # boxes\n",
    "        boxes = torch.tensor([annot['bbox'] for annot in annots], dtype=torch.float32)\n",
    "        boxes_xyxy = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        boxe_tv = tv_tensors.BoundingBoxes(boxes_xyxy, format= 'XYXY', canvas_size=(h,w))\n",
    "        # masks\n",
    "        segmentations = [annot['segmentation'] for annot in annots]\n",
    "        masks = []\n",
    "        for seg in segmentations:\n",
    "            rle = maskUtils.merge(maskUtils.frPyObjects(seg, h, w))\n",
    "            mask = maskUtils.decode(rle)\n",
    "            masks.append(torch.from_numpy(mask))\n",
    "        mask_tv = tv_tensors.Mask(torch.stack(masks))\n",
    "\n",
    "        target = {}\n",
    "        target['image_id'] = img_id\n",
    "        target['labels'] = labels\n",
    "        target['area'] = areas\n",
    "        target['boxes'] = boxe_tv\n",
    "        target['masks'] = mask_tv\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557b52b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf025fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "# TODO check (sample code)\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7c2f6",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c301cbb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430a98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_transform(img_size: Tuple[int, int]|None = None):\n",
    "    transforms = []\n",
    "    if img_size is not None:\n",
    "        transforms.append(T.Resize(img_size))\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to prevent stacking of images with different shapes\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_cat_ids(cats, names):\n",
    "    \"\"\"\n",
    "    Get categories ids with specific names\n",
    "    \"\"\"\n",
    "    cat_ids = []\n",
    "    cat_names = []\n",
    "    for id, cat in cats.items():\n",
    "        if cat['name'] in names:\n",
    "            cat_ids.append(id)\n",
    "            cat_names.append(cat['name'])\n",
    "    print(f'category found for {[name for name in names if name in cat_names]}')\n",
    "    print(f'category NOT found for {[name for name in names if name not in cat_names]}\\n')\n",
    "    return cat_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19293e22",
   "metadata": {},
   "source": [
    "#### Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeaca598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImagesPlotReady(msk, shape):\n",
    "    return (np.rot90(np.rot90(np.rot90((msk.reshape(shape[0],shape[1],shape[2])).T))))\n",
    "\n",
    "\n",
    "def show_image(image): \n",
    "    _image = image if image.ndim == 3 else image[0]\n",
    "    _, w, h = _image.size()\n",
    "    plt.figure(figsize=(5*(w/h),5*(h/w)))\n",
    "    _image = makeImagesPlotReady(_image, _image.size())\n",
    "    plt.imshow(_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_list(images):\n",
    "    n_images = len(images)\n",
    "    images = [makeImagesPlotReady(img, img.size()) for img in images]\n",
    "    _, axs = plt.subplots(1, n_images, figsize=(3*n_images, 3))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(n_images):\n",
    "        axs[i].imshow(images[i])\n",
    "        axs[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8b1a4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178314fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_one_epoch(model, optmizer, data_loader, device, print_freq=1):\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, targets) in tqdm(enumerate(data_loader)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        losses.backward()\n",
    "        optmizer.step()\n",
    "        if i%print_freq==0:\n",
    "            print(loss_value)\n",
    "            \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "from lvis import LVISResults\n",
    "from lvis import LVISEval\n",
    "\n",
    "# TODO not working properly\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, lvis_gt, device):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        output = model(images)\n",
    "        output = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in output]\n",
    "        output = [t | {'image_id': targets[i]['image_id']} for i, t in enumerate(output)]\n",
    "        outputs.extend(output)\n",
    "    print(len(outputs))\n",
    "    print(len(outputs[0]))\n",
    "    print(outputs[0].keys())\n",
    "    lvis_dt = LVISResults(lvis_gt, outputs)\n",
    "    lvis_eval = LVISEval(lvis_gt, lvis_dt)\n",
    "    lvis_eval.run()\n",
    "    lvis_eval.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5a34",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf10d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data args\n",
    "CATEGORIES = ['cat', 'dog'] #, 'cow', 'pigeon', 'giraffe', 'bear']\n",
    "IMG_SIZE = None     # if None: no resize\n",
    "\n",
    "# Learning args\n",
    "BATCH_SIZE = 5\n",
    "EPOCHS = 2\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d3fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category found for ['cat', 'dog']\n",
      "category NOT found for []\n",
      "\n",
      "category found for ['cat', 'dog']\n",
      "category NOT found for []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lvis import LVIS\n",
    "\n",
    "lvis_gt_train = FilteredLVIS(TRAIN_ANNOT_PATH, CATEGORIES)\n",
    "lvis_gt_val = FilteredLVIS(VAL_ANNOT_PATH, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3224df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category found for ['cat', 'dog']\n",
      "category NOT found for []\n",
      "\n",
      "will load images from files\n",
      "Size of train dataset: 3819\n",
      "will load images from files\n",
      "Size of validation dataset: 745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "cat_ids = get_cat_ids(lvis_gt_train.cats, CATEGORIES)\n",
    "num_classes = len(cat_ids) + 1 # +1 for background\n",
    "\n",
    "train_dataset = LVISDataset([COCO2017_TRAIN_PATH], lvis_gt_train, get_transform(IMG_SIZE), cat_ids= cat_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, shuffle=True)\n",
    "print(f\"Size of train dataset: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = LVISDataset([COCO2017_TRAIN_PATH,COCO2017_VAL_PATH], lvis_gt_val, get_transform(IMG_SIZE), cat_ids= cat_ids)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "print(f\"Size of validation dataset: {len(val_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# TODO check (sample code)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# TODO check (sample code)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# TODO\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_value = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    # evaluate(model, val_loader, lvis_gt_val, device)\n",
    "    lr_scheduler.step()\n",
    "    print(loss_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e616340",
   "metadata": {},
   "source": [
    "# --- tests ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95122b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "show_image_list(images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lvis import LVIS, LVISVis\n",
    "\n",
    "lvis_gt = LVIS(VAL_ANNOT_PATH)\n",
    "\n",
    "lvis_vis = LVISVis(lvis_gt, img_dir = COCO2017_VAL_PATH)\n",
    "\n",
    "lvis_vis.vis_img(285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a46851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training output\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "images, targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference output\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "val_subset = Subset(val_dataset, indices=range(10))\n",
    "val_subset_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "evaluate(model, val_subset_loader, lvis_gt_val, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
