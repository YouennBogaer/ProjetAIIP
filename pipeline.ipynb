{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdb81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.406824Z",
     "start_time": "2025-11-21T10:50:41.550961Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix old numpy code in LVISEval (np.float is deprecated)\n",
    "if not hasattr(np, \"float\"):\n",
    "    np.float = float\n",
    "import torch\n",
    "import torchvision\n",
    "import lvis\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from constants import *\n",
    "from typing import List, Tuple, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e1043",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb267844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.443628Z",
     "start_time": "2025-11-21T10:50:48.418690Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.ops import box_convert\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "class LVISDataset(Dataset):\n",
    "    def __init__(self, img_dirs: List[str], lvis_gt: lvis.LVIS, transforms=None,\n",
    "                 cat_ids=None) -> None:\n",
    "        self.img_dirs = img_dirs\n",
    "        self.lvis_gt = lvis_gt\n",
    "        self.transforms = transforms\n",
    "        self._create_index(cat_ids) \n",
    "\n",
    "    def _create_index(self, cat_ids):\n",
    "        self.img_ids = self.lvis_gt.get_img_ids() if cat_ids is None else self._get_img_ids(cat_ids)\n",
    "        self.cat_ids = self.lvis_gt.get_cat_ids() if cat_ids is None else cat_ids\n",
    "        self.cat_id_to_label = {cat_id: i+1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "        self.label_to_cat_id = {i+1: cat_id for i, cat_id in enumerate(self.cat_ids)}\n",
    "        if  all(isinstance(dir, str) for dir in self.img_dirs) and all(os.path.isdir(dir) for dir in self.img_dirs):\n",
    "            self._get_image = self._get_image_from_file\n",
    "            print(\"will load images from files\")\n",
    "        else:\n",
    "            self._get_image = self._get_image_from_url\n",
    "            print(\"will load images from urls\")\n",
    "\n",
    "    def _get_img_ids(self, cat_ids):\n",
    "        return list({\n",
    "            iid for cat_id in cat_ids\n",
    "            for iid in self.lvis_gt.cat_img_map[cat_id]\n",
    "        })\n",
    "    \n",
    "    def _get_image_from_file(self, id):\n",
    "        image_paths = [os.path.join(images_dir, f'{str(id).zfill(12)}.jpg') for images_dir in self.img_dirs]\n",
    "        for image_path in image_paths:\n",
    "            if os.path.isfile(image_path):\n",
    "                return decode_image(image_path)\n",
    "        print(f\"image not found: {image_paths}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    def _get_image_from_url(self, id):\n",
    "        url = self.lvis_gt.imgs[id]['coco_url']\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ### get image\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = self._get_image(img_id)\n",
    "        img = tv_tensors.Image(img)\n",
    "        _, h, w = img.shape\n",
    "\n",
    "        ### get annotationss\n",
    "        annot_ids = self.lvis_gt.get_ann_ids(img_ids=[img_id])\n",
    "        annots = self.lvis_gt.load_anns(annot_ids)\n",
    "        annots = [annot for annot in annots if annot['category_id'] in self.cat_ids]\n",
    "        # labels\n",
    "        labels = torch.tensor([self.cat_id_to_label[annot['category_id']] for annot in annots])\n",
    "        # area\n",
    "        areas = torch.tensor([annot['area'] for annot in annots])\n",
    "        # boxes\n",
    "        boxes = torch.tensor([annot['bbox'] for annot in annots], dtype=torch.float32)\n",
    "        boxes_xyxy = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        boxe_tv = tv_tensors.BoundingBoxes(boxes_xyxy, format= 'XYXY', canvas_size=(h,w)) # type: ignore\n",
    "        # masks\n",
    "        masks = [torch.from_numpy(self.lvis_gt.ann_to_mask(ann)) for ann in annots] # shape: (N, H, W)\n",
    "        mask_tv = tv_tensors.Mask(torch.stack(masks))\n",
    "\n",
    "        target = {}\n",
    "        target['image_id'] = img_id\n",
    "        target['labels'] = labels\n",
    "        target['area'] = areas\n",
    "        target['boxes'] = boxe_tv\n",
    "        target['masks'] = mask_tv\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557b52b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf025fc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.454142Z",
     "start_time": "2025-11-21T10:50:48.449189Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "# with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "# TODO check (sample code)\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features # type: ignore\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels # type: ignore\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7c2f6",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c301cbb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a98f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.499355Z",
     "start_time": "2025-11-21T10:50:48.458077Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_transform(img_size: Tuple[int, int] | None = None):\n",
    "    transforms = []\n",
    "    if img_size is not None:\n",
    "        transforms.append(T.Resize(img_size))\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to prevent stacking of images with different shapes\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_filtered_cat_ids(cats, names):\n",
    "    \"\"\"\n",
    "    Get a list of ids of given list of categories\n",
    "    \"\"\"\n",
    "    cat_ids = []\n",
    "    cat_names = []\n",
    "    for id, cat in cats.items():\n",
    "        if cat['name'] in names:\n",
    "            cat_ids.append(id)\n",
    "            cat_names.append(cat['name'])\n",
    "    print(f'category found for {[name for name in names if name in cat_names]}')\n",
    "    print(f'category NOT found for {[name for name in names if name not in cat_names]}\\n')\n",
    "    return cat_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8b1a4",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178314fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.519414Z",
     "start_time": "2025-11-21T10:50:48.507125Z"
    }
   },
   "outputs": [],
   "source": [
    "from lvis import LVISResults, LVISEval\n",
    "import pycocotools.mask as maskUtils\n",
    "import math\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, epoch, device, print_freq:None|int =10):\n",
    "    model.train()\n",
    "    total_batches = len(data_loader)\n",
    "    lr_scheduler = None\n",
    "    if epoch==0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "    total_loss = 0\n",
    "    for i, (images, targets) in tqdm(enumerate(data_loader), total=total_batches, desc=\"TRAIN EPOCH (/batches)\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        total_loss += loss_value\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if print_freq is not None and i % print_freq == 0:\n",
    "            tqdm.write(f\"[batch {i+1}/{total_batches}] loss: {loss_value}\")\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training \\nLoss dict:\\n{loss_dict}\")\n",
    "            sys.exit(1)\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    return total_loss / total_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_val_loss(model, data_loader, device):\n",
    "    model.train()\n",
    "    total_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    for images, targets in tqdm(data_loader, total=total_batches, desc=\"VALIDATION (/batches)\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss += sum(loss.item() for loss in loss_dict.values())\n",
    "    return total_loss / total_batches\n",
    "\n",
    "\n",
    "# TODO masks do not seem to be in the right format\n",
    "@torch.inference_mode()\n",
    "def get_predictions(model, data_loader, device, score_thresh=0.5):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for images, targets in tqdm(data_loader, total=len(data_loader), desc=\"BATCHES\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "        for o, t in zip(outputs, targets):\n",
    "            image_id = t[\"image_id\"]\n",
    "            boxes = o[\"boxes\"].cpu().numpy()\n",
    "            scores = o[\"scores\"].cpu().numpy()\n",
    "            labels = o[\"labels\"].cpu().numpy()\n",
    "            masks = o[\"masks\"].cpu().numpy()[:, 0, :, :]\n",
    "            for box, score, label, mask in zip(boxes, scores, labels, masks):\n",
    "                if score_thresh is not None and score < score_thresh:\n",
    "                    continue\n",
    "                mask_bin = (mask > 0.5).astype(np.uint8)\n",
    "                rle = maskUtils.encode(np.asfortranarray(mask_bin))\n",
    "                rle['counts'] = rle['counts'].decode('utf-8')\n",
    "                x1, y1, x2, y2 = box\n",
    "                predictions.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": data_loader.dataset.dataset.label_to_cat_id[int(label)], # dict inside dataset in subset in loader\n",
    "                    \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                    \"segmentation\":rle,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, lvis_gt, cat_ids, device, iou_type:Literal[\"bbox\",\"segm\"]=\"segm\", score_thresh=0.5):\n",
    "    predictions = get_predictions(model, data_loader, device, score_thresh)\n",
    "    if len(predictions) == 0:\n",
    "        print(\"No detections — skipping LVIS evaluation.\")\n",
    "    else:\n",
    "        lvis_dt = LVISResults(lvis_gt, predictions)\n",
    "        lvis_eval = LVISEval(lvis_gt, lvis_dt, iou_type)\n",
    "        lvis_eval.params.cat_ids = cat_ids\n",
    "        lvis_eval.run()\n",
    "        lvis_eval.print_results()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, optimizer, lr_scheduler, \n",
    "    train_loader, val_loader, lvis_gt_val, cat_ids, \n",
    "    epochs, patience,\n",
    "    device, print_freq:None|int =10\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improvement = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc= 'TRAINING LOOP (/epochs)'):\n",
    "        loss_train = train_one_epoch(model, optimizer, train_loader, epoch, device, print_freq=print_freq)\n",
    "        train_losses.append(loss_train)\n",
    "\n",
    "        #evaluate(model, val_loader, lvis_gt_val, cat_ids, device)\n",
    "        loss_val = compute_val_loss(model, val_loader, device)\n",
    "        val_losses.append(loss_val)\n",
    "\n",
    "        tqdm.write(f\"[epoch {epoch}/{epochs}]: train loss = {loss_train} | val loss = {loss_val}\")\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_model_epoch = epoch\n",
    "            epochs_no_improvement = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            epochs_no_improvement += 1\n",
    "            tqdm.write(f\"NO improvement [{epochs_no_improvement}/{patience}]\")\n",
    "            if epochs_no_improvement > patience:\n",
    "                print(\"Patience reached, stopping training\")\n",
    "                break\n",
    "        lr_scheduler.step()\n",
    "    return train_losses, val_losses, best_model_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc4832",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61488387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImagesPlotReady(msk, shape):\n",
    "    return (np.rot90(np.rot90(np.rot90((msk.reshape(shape[0], shape[1], shape[2])).T))))\n",
    "\n",
    "def show_image(image):\n",
    "    _image = image if image.ndim == 3 else image[0]\n",
    "    _, w, h = _image.size()\n",
    "    plt.figure(figsize=(5 * (w / h), 5 * (h / w)))\n",
    "    _image = makeImagesPlotReady(_image, _image.size())\n",
    "    plt.imshow(_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_image_list(images):\n",
    "    n_images = len(images)\n",
    "    images = [makeImagesPlotReady(img, img.size()) for img in images]\n",
    "    _, axs = plt.subplots(1, n_images, figsize=(3 * n_images, 3))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(n_images):\n",
    "        axs[i].imshow(images[i])\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(train_losses, val_losses, early_stop = None, filepath=None):\n",
    "    _, ax = plt.subplots(figsize=((12, 6)))\n",
    "    epochs = np.arange(1,len(train_losses)+1,1)\n",
    "    ax.plot(epochs, train_losses, 'r', label='Training Loss')\n",
    "    ax.plot(epochs, val_losses, 'g', label='Validation Loss')\n",
    "    if early_stop is not None:\n",
    "        plt.scatter(epochs[early_stop], val_losses[early_stop], marker='x', c='g', label='Saved Model Epoch')\n",
    "    ax.set_title('Loss Plots')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    if filepath is not None: \n",
    "        plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5a34",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22da0f",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10d76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:50:48.524397Z",
     "start_time": "2025-11-21T10:50:48.520667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data args\n",
    "MAX_IMAGES = 20 # -1:all & max_image_val = max_image_train // 5 (80/20 split)\n",
    "CATEGORIES = ['cat', 'dog']  #, 'cow', 'pigeon', 'giraffe', 'bear']\n",
    "\n",
    "#We absolutely need a resize or use a btach of one\n",
    "IMG_SIZE = None #(256, 256)  # if None: no resize\n",
    "\n",
    "# Learning args\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 3\n",
    "PATIENCE = 5\n",
    "DEVICE = 'cpu'\n",
    "NUM_WORKERS = 0 # int (0: main process)\n",
    "BATCH_PRINT_FREQ = 1 # int|None (None: no print inside epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105a186",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3fc3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:51:50.652372Z",
     "start_time": "2025-11-21T10:50:48.525395Z"
    }
   },
   "outputs": [],
   "source": [
    "from lvis import LVIS\n",
    "\n",
    "lvis_gt_train = LVIS(TRAIN_ANNOT_PATH) #, CATEGORIES)\n",
    "lvis_gt_val = LVIS(VAL_ANNOT_PATH) #, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3224df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:51:50.724465Z",
     "start_time": "2025-11-21T10:51:50.662427Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "cat_ids = get_filtered_cat_ids(lvis_gt_train.cats, CATEGORIES)\n",
    "num_classes = len(cat_ids) + 1  # +1 for background\n",
    "\n",
    "dataset_train = LVISDataset([COCO2017_TRAIN_PATH], lvis_gt_train, get_transform(IMG_SIZE), cat_ids=cat_ids)\n",
    "subset_train = Subset(dataset_train, (torch.randperm(len(dataset_train))[:MAX_IMAGES]).tolist())\n",
    "train_loader = DataLoader(subset_train, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(f\"Size of train dataset: {len(dataset_train)}\")\n",
    "print(f\"Size of train subset: {len(subset_train)}\\n\")\n",
    "\n",
    "dataset_val = LVISDataset([COCO2017_VAL_PATH, COCO2017_TRAIN_PATH], lvis_gt_val, get_transform(IMG_SIZE), cat_ids=cat_ids)\n",
    "subset_val = Subset(dataset_val, (torch.randperm(len(dataset_val))[:MAX_IMAGES//5]).tolist())\n",
    "val_loader = DataLoader(subset_val, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, num_workers=NUM_WORKERS)\n",
    "print(f\"Size of validation dataset: {len(dataset_val)}\")\n",
    "print(f\"Size of validation subset: {len(subset_val)}\")\n",
    "\n",
    "# for faster evaluation (when instancing LVISResults):\n",
    "lvis_gt_val.cats= {k: v for k, v in lvis_gt_val.cats.items() if k in cat_ids}\n",
    "lvis_gt_train.cats= {k: v for k, v in lvis_gt_train.cats.items() if k in cat_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca722e51",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2c17c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:53:19.687884Z",
     "start_time": "2025-11-21T10:51:50.727469Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# TODO check (sample code)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "# TODO check (sample code)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "train_losses, val_losses, best_epoch = train(model, \n",
    "                                             optimizer, lr_scheduler, \n",
    "                                             train_loader, val_loader, lvis_gt_val, cat_ids,\n",
    "                                             EPOCHS, PATIENCE, device, BATCH_PRINT_FREQ)\n",
    "plot_losses(train_losses, val_losses, early_stop = best_epoch, filepath=\"losses.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945571",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_loader, lvis_gt_train, cat_ids, device, iou_type=\"segm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e616340",
   "metadata": {},
   "source": [
    "# --- tests ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95122b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "show_image_list(images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lvis import LVIS, LVISVis\n",
    "\n",
    "predictions = get_predictions(model, val_loader, device)\n",
    "if len(predictions) == 0:\n",
    "    print(\"No detections — skipping LVIS evaluation.\")\n",
    "else:\n",
    "    lvis_dt = LVISResults(lvis_gt_val, predictions)\n",
    "\n",
    "lvis_vis = LVISVis(lvis_gt_val, lvis_dt, img_dir=COCO2017_VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lvis_dt.anns))\n",
    "i = 2\n",
    "print(lvis_dt.anns[i][\"image_id\"])\n",
    "print(lvis_dt.anns[i][\"area\"])\n",
    "print(np.sum(lvis_dt.anns[i][\"segmentation\"]))\n",
    "\n",
    "lvis_vis.vis_result(222317, show_boxes=True, show_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lvis import LVIS, LVISVis\n",
    "\n",
    "lvis_gt = LVIS(VAL_ANNOT_PATH)\n",
    "\n",
    "lvis_vis = LVISVis(lvis_gt, img_dir=COCO2017_VAL_PATH)\n",
    "\n",
    "lvis_vis.vis_img(285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a46851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training output\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "print(\"debug 1\")\n",
    "images, targets = next(iter(train_loader))\n",
    "print(\"debug 2\")\n",
    "images = list(image for image in images)\n",
    "print(\"debug 3\")\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "print(\"debug 4\")\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(\"debug 5\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference output\n",
    "@torch.inference_mode()\n",
    "def test(model) :\n",
    "    model.eval()\n",
    "    print(\"debug 1\")\n",
    "    x = [torch.rand(3, 300, 400).to(device), torch.rand(3, 500, 400).to(device)]\n",
    "    print(\"debug 2\")\n",
    "    predictions = model(x)  # Returns predictions\n",
    "    print(\"debug 3\")\n",
    "    print(predictions[0])\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "val_subset = Subset(dataset_val, indices=range(10))\n",
    "val_subset_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device)\n",
    "for images, targets in val_loader:\n",
    "    print(type(targets), targets)\n",
    "    break\n",
    "    \n",
    "print(\"collate_fn =\", val_loader.collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "evaluate(model, val_loader, lvis_gt_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e40bcaf784133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"../coco2017/train2017/000000055296.jpg\")\n",
    "if img is None : \n",
    "    print(\"erreur imgage pas found\")\n",
    "cv2.imshow(\"test\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef40d04a576b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/lvis_v1_val.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "category_fields = list(data[\"categories\"][0].keys())\n",
    "\n",
    "print(category_fields)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
